1) Возьмите два массива маленьких текстов, например новости про политику и новости про искуссство. (Подсказка: например, новости с newsru.com, если их копипастить, можно разбивать split()'ом по символу ":", чтобы получить примерный эквивалент разделения на отдельные новости)
2) Постройте для них морфологический анализ (например, с помощью mystem -d -n -i -s --format=json --eng-gr in.txt out.txt) (подсказка: соответственно, разбивать текст на части удобнее после mystem, а не до)
3) Прочитайте массивы как списки текстов
4) В каждом тексте постройте число вхождений каждой части речи
5) Постройте таблицу, в которой для каждого предложения для каждой части речи указано количество находок этой части речи в этом предложении
6) Присоедините к таблице таблицу векторов tf-idf для этих текстов
7) Создайте массив длины, равной суммарной длине таблиц, содержащий номер коллекции. (Это будет значение Y для машинного обучения). (подсказка: штука в духе [0]*len(table1) + [1]*len(table2) делает то, что нужно)
8) Отправьте данные на вход grid_search.GridSearchCV(svm.SVC(), {}) и grid_search.GridSearchCV(naive_bayes.MultinomialNB(), {})
9) Покажите процент успеха (best_score_)
10) Возьмите наилучший из получившихся классификаторов (best_estimator_), и с его помощью найдите 3 примера, где машина угадывает, и 3 примера, где машина ошибается
